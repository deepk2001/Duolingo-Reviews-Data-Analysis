{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdWSm2r7OyvYXlsaezs/ma",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepk2001/Duolingo-Reviews-Data-Analysis/blob/main/Practicum_Duolingo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "We have chosen **Duolingo** app on Google Play Store for our Practicum for the following reasons:\n",
        "\n",
        "1. Popular app, therefore has an abundance of reviews and users\n",
        "2. Familiarity, the team is familiar with Duolingo's operations and therefore can really relate to user's reviews about the app\n",
        "3. Regular updates, The app has rolled out regular patch, minor and the occasional major updates.\n",
        "\n",
        "Below we have code that performs the basic data collection using a popular library for scraping data on google play store."
      ],
      "metadata": {
        "id": "ytOPBTqqDgWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Packges"
      ],
      "metadata": {
        "id": "1tbu8fA4jx03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njii8VYJhKsJ"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install google-play-scraper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch reviews and store in CSV\n",
        "To fetch the data we use the given library function and we fetch 60,000 reviews sorted by relevance.\n",
        "\n",
        "**Note**: We have elected to use relevance as a sorting feature to be able to ensure that we can have a good balance between review types and relevant reviews may have more actionable information.\n",
        "\n",
        "The returned reviews are plugged into a Data Frame with focus on base columns like ['userName', 'score', 'content', 'at', 'appVersion'].\n",
        "\n",
        "These columns were picked as they have provided vital information regarding the reviews left by app users.\n"
      ],
      "metadata": {
        "id": "zvy-uJllj4G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google_play_scraper import reviews, Sort\n",
        "import pandas as pd\n",
        "\n",
        "app_id = \"com.duolingo\"  # Duolingo app ID\n",
        "\n",
        "def fetch_reviews_by_sort(app_id, sort_type, count=1000):\n",
        "    result, _ = reviews(\n",
        "        app_id=app_id,\n",
        "        lang='en',\n",
        "        country='us',\n",
        "        sort=sort_type,\n",
        "        count=count\n",
        "    )\n",
        "\n",
        "    df = pd.DataFrame(result)\n",
        "    df['sort_type'] = sort_type.name  # Track source\n",
        "    return df\n",
        "\n",
        "\n",
        "# Fetch most relevant reviews\n",
        "df = fetch_reviews_by_sort(app_id, Sort.MOST_RELEVANT, count=60000)\n",
        "\n",
        "columns =['userName', 'score', 'content', 'at', 'appVersion']\n",
        "\n",
        "# Keep relevant columns\n",
        "df = df[columns]\n",
        "\n",
        "df.to_csv(\"duolingo_reviews.csv\", index=False)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "lRVBiRfqhbT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "\n",
        "To clean the data, we have followed the following steps:\n",
        "1. Removal of duplicate reviews.\n",
        "2. We remove any data that has a nullable value within any of our base columns.\n",
        "3. we create a new column cleaned_content that is the content column without the stopwords and any links. We also convert the text into lowercase to standardize it.\n",
        "4. We once again remove any data points that have nullable cleaned_content."
      ],
      "metadata": {
        "id": "2rlYm_DLkDa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicates\n",
        "df.drop_duplicates(subset='content', inplace=True)\n",
        "\n",
        "# remove null values\n",
        "\n",
        "for column in columns:\n",
        "  df = df[df[column].notnull()]\n",
        "\n",
        "#standardize text\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_content'] = df['content'].apply(clean_text)\n",
        "\n",
        "#clean review out that have no cleaned_content\n",
        "df = df[df[\"cleaned_content\"].notnull()]\n",
        "df = df[df['cleaned_content'].str.strip() != \"\"]\n",
        "\n",
        "\n",
        "#make sure date is in datetime format\n",
        "\n",
        "df['at'] = pd.to_datetime(df['at'])\n",
        "dateRange = (df[\"at\"].min(), df['at'].max())\n",
        "versionRange = (df[\"appVersion\"].min(),df[\"appVersion\"].max())\n",
        "print( \"Time Interval of data: \",dateRange)\n",
        "print(\"Number of Data Points: \", len(df))\n",
        "print(\"Version Interval of data: \", versionRange)\n"
      ],
      "metadata": {
        "id": "HR-niTj4kIs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "9voTYzuL5Ros"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring and Analysing the data\n",
        "\n",
        "1. Word Frequency:\n",
        "We look at the word frequency by analysing the 50 most frequent words. We can already understand that the frequency of neutral and positive words will dominate. But we are more interested in understanding the common themes in user reviews."
      ],
      "metadata": {
        "id": "zpfVDTRe5G0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    stop_words='english',\n",
        "    max_features=50\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(df['cleaned_content'])\n",
        "word_counts = X.sum(axis=0)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "freq_df = pd.DataFrame({\n",
        "    'word': words,\n",
        "    'frequency': word_counts.tolist()[0]\n",
        "}).sort_values(by='frequency', ascending=False)\n",
        "\n",
        "print(freq_df)"
      ],
      "metadata": {
        "id": "OBP2KH3C5OiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Frequency findings:\n",
        "\n",
        "1. As expected the words are more over postive and neutral domain. However some themes have begun to emerge.\n",
        "2. Energy is very common word, contextually implying that a common theme of reviews is the energy feature introduced in version 6 and after.\n",
        "3. other words like ads and pay imply issues regarding the experieces with these instances of the app.\n"
      ],
      "metadata": {
        "id": "0g9m9bLZNznV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "five_months_ago = datetime.now() - timedelta(days=150)\n",
        "recent_reviews = df[df['at'] >= five_months_ago]"
      ],
      "metadata": {
        "id": "XJUKDJVO6crF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['month'] = df['at'].dt.to_period('M')\n",
        "monthly_counts = df.groupby('month').size()\n",
        "print(monthly_counts)"
      ],
      "metadata": {
        "id": "KyfHN1f56dsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monthly reviews count:\n",
        "\n",
        "1. The reviews have spiked recently as compared to before.\n",
        "2. The spike is disproportionate with respect to increase in users.\n",
        "3. The recent updates have excited the users."
      ],
      "metadata": {
        "id": "OFKB6CXIPNKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_rating = df.groupby('month')['score'].mean()\n",
        "print(monthly_rating)"
      ],
      "metadata": {
        "id": "Y6kRA1-S6nZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "\n",
        "To analyze user reviews, the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis model was selected due to its suitability for short, informal text data such as mobile application reviews.\n",
        "\n",
        "VADER is a lexicon and rule-based sentiment analysis tool specifically designed for social media and user-generated content. Unlike traditional machine learning models, it does not require labeled training data, making it computationally efficient and easy to implement for large-scale review datasets.\n",
        "\n",
        "One of the primary reasons for choosing VADER is its ability to handle linguistic nuances commonly found in app reviews, including:\n",
        "\n",
        "- Emphasis through capitalization (e.g., \"THIS UPDATE IS TERRIBLE\")\n",
        "- Repeated punctuation (e.g., \"awful!!!\")\n",
        "- Degree modifiers (e.g., \"very bad\", \"extremely slow\")\n",
        "- Negations (e.g., \"not good\")\n",
        "- Informal language and short expressions\n",
        "\n",
        "VADER produces four sentiment metrics: positive, negative, neutral, and a compound score. The compound score ranges from -1 to +1 and represents the normalized overall sentiment of a review.\n",
        "\n",
        "In this study, the compound score was used for classification as follows:\n",
        "\n",
        "- Compound â‰¥ 0.05 â†’ Positive  \n",
        "- Compound â‰¤ -0.05 â†’ Negative  \n",
        "- Otherwise â†’ Neutral  \n",
        "\n",
        "The compound score was particularly useful for downstream analysis, including clustering negative reviews and identifying major complaint themes across specific time periods and app version updates.\n",
        "\n",
        "Given the nature of the datasetâ€”short, emotionally expressive app reviewsâ€”VADER provides a reliable, interpretable, and computationally efficient approach for sentiment classification."
      ],
      "metadata": {
        "id": "LvmGv5e5_Qmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "4v26oRI3_YYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "df['sentiment_score'] = df['cleaned_content'].apply(get_sentiment)\n",
        "\n",
        "def sentiment_label(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "df['sentiment'] = df['sentiment_score'].apply(sentiment_label)\n",
        "negative_reviews = df[df['sentiment'] == 'Negative'].copy()\n",
        "neutral_reviews = df[df['sentiment'] == 'Neutral'].copy()\n",
        "positive_reviews = df[df['sentiment'] == 'Positive'].copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "16rQcZsi_Uyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribution of Reviews\n",
        "\n",
        "Based on the sentiment assigned by our parameters at Vader. The next step is to visualize the reviews and understand any focus regions that can help us understand the negative review themes better."
      ],
      "metadata": {
        "id": "7cYHoI1QDxA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count sentiment categories\n",
        "sentiment_counts = df['sentiment'].value_counts()\n",
        "\n",
        "# Ensure consistent order\n",
        "sentiment_counts = sentiment_counts.reindex(['Positive', 'Neutral', 'Negative']).fillna(0)\n",
        "\n",
        "colors = ['green', 'yellow', 'red']\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure()\n",
        "plt.bar(sentiment_counts.index, sentiment_counts.values, color=colors)\n",
        "\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.title(\"Sentiment Distribution of Reviews\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vUyUiYlmD08r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Distribution Analysis:\n",
        "\n",
        "1. The simple count distribution suggests that majority of app users have been in favor of the app updates and experience.\n",
        "2. A major focus is that there is a lack of Neutral reviews, which means that the average user is either content with the app or incontent.\n",
        "3. We need to plot it based on version and months to understand the trends better."
      ],
      "metadata": {
        "id": "YNKZQGlZVbhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['at'] = pd.to_datetime(df['at'])\n",
        "\n",
        "# Extract month (year-month)\n",
        "df['month'] = df['at'].dt.to_period('M')\n",
        "\n",
        "# Count of each sentiment per month\n",
        "monthly_sentiment = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "# Ensure consistent order for sentiment columns\n",
        "monthly_sentiment = monthly_sentiment[['Positive', 'Neutral', 'Negative']]\n",
        "\n",
        "# Plot\n",
        "monthly_sentiment.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    color=['green', 'yellow', 'red'],\n",
        "    figsize=(20,6)\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.title(\"Monthly Sentiment Distribution\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Sentiment')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jwG1Om-lFSQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monthly Distribution Analysis:\n",
        "1. The spike in recent reviewer activity is very evident.\n",
        "2. What is also to be understood is that recently the number of negative reviews have increased manyfold."
      ],
      "metadata": {
        "id": "qr_FILhTWHEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
        "\n",
        "# Ensure datetime\n",
        "df['at'] = pd.to_datetime(df['at'])\n",
        "\n",
        "# Extract month\n",
        "df['month'] = df['at'].dt.to_period('M')\n",
        "\n",
        "# Count sentiment per month\n",
        "monthly_sentiment = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)\n",
        "monthly_sentiment = monthly_sentiment[['Positive', 'Neutral', 'Negative']]\n",
        "\n",
        "# Calculate average rating per month\n",
        "avg_rating = df.groupby('month')['score'].mean().reindex(monthly_sentiment.index)\n",
        "\n",
        "# Define gradient colormap: red -> orange -> yellow -> light green -> dark green\n",
        "colors = ['red', 'orange', '#a88100', 'lightgreen', 'green']\n",
        "cmap = LinearSegmentedColormap.from_list('rating_cmap', colors)\n",
        "\n",
        "# Normalize rating for colormap\n",
        "norm = Normalize(vmin=avg_rating.min(), vmax=avg_rating.max())\n",
        "rating_colors = [cmap(norm(r)) for r in avg_rating]\n",
        "\n",
        "# Plot stacked bar\n",
        "ax = monthly_sentiment.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    color=['green', 'yellow', 'red'],\n",
        "    figsize=(20,6),\n",
        "    width=0.8\n",
        ")\n",
        "\n",
        "# Annotate average rating on top with gradient colors\n",
        "for i, rating in enumerate(avg_rating):\n",
        "    total_height = monthly_sentiment.iloc[i].sum()\n",
        "    ax.text(\n",
        "        i,\n",
        "        total_height + 2,  # slightly above bar\n",
        "        f\"{rating:.2f}\",\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=6,\n",
        "        fontweight='bold',\n",
        "        color=rating_colors[i]\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.title(\"Monthly Sentiment Distribution with Average Rating Colored by Value\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Sentiment')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kp5P5igaNZDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monthly Distribution with Average App rating:\n",
        "The chart shows the monthly distribution of Positive, Neutral, and Negative reviews using a stacked bar format. Each bar represents total review volume, while segment sizes indicate the proportion of each sentiment category. The average monthly rating is displayed above each bar and color-coded from red (lower ratings) to green (higher ratings) for quick visual interpretation. Overall, the plot highlights shifts in user sentiment and provides a clear view of how overall ratings align with sentiment trends over time.\n",
        "\n",
        "Points of focus:\n",
        "1. Recently the app has recieved some heavy criticisms.\n",
        "2. these criticisms are consistent with both sentiment analysis and trends in the rating of the app.\n",
        "3. highest impact is between 2025-07 to 2026-02.\n",
        "4. Let us focus more in this region."
      ],
      "metadata": {
        "id": "C0t95NGhYDaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract minor version (major + minor)\n",
        "df['major_version'] = df['appVersion'].str.split('.').str[0]\n",
        "\n",
        "# Count sentiment per minor version\n",
        "minor_sentiment = df.groupby(['major_version', 'sentiment']).size().unstack(fill_value=0)\n",
        "minor_sentiment = minor_sentiment[['Positive', 'Neutral', 'Negative']]\n",
        "print(df.head())\n",
        "# Sort numerically\n",
        "minor_sentiment = minor_sentiment.sort_index(\n",
        "    key=lambda x: x.map(lambda y: [int(i) for i in y.split('.')])\n",
        ")\n",
        "\n",
        "# Plot\n",
        "minor_sentiment.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    color=['green', 'yellow', 'red'],\n",
        "    figsize=(16,6),\n",
        "    width=0.8\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Major Version\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.title(\"Sentiment Distribution by Major Version\")\n",
        "plt.legend(title='Sentiment')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HwXlnw79H5mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version Distribution Analysis:\n",
        "1. version 6 has recieved the most negative reviews.\n",
        "2. This ratio suggests that the new updates to the UX have impacted some users in an undesired manner.\n",
        "3. The reason for selecting major version updates over minor or patch is that as per Duo-lingo's documentation, they only make major shifts to the UX in major versions.\n"
      ],
      "metadata": {
        "id": "eGUZYy56dIJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['at'] = pd.to_datetime(df['at'])\n",
        "\n",
        "# Filter negative reviews\n",
        "negative_reviews = df[df['sentiment'] == 'Negative'].copy()\n",
        "\n",
        "# Define date range\n",
        "start_date = pd.Timestamp('2025-07-01')\n",
        "end_date   = pd.Timestamp('2026-02-28')\n",
        "\n",
        "# Filter by date\n",
        "negative_reviews_range = negative_reviews[\n",
        "    (negative_reviews['at'] >= start_date) & (negative_reviews['at'] <= end_date)\n",
        "]\n",
        "\n",
        "# Sort by sentiment_score ascending (most negative first)\n",
        "negative_reviews_range_sorted = negative_reviews_range.sort_values(by='sentiment_score', ascending=True)\n",
        "\n",
        "negative_reviews_range_sorted.to_csv(\"duolingo_reviews_negative_spike.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "VNJMiv_sTd0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Focus Region\n",
        "Here the focus region is updated to include only Negative reviews from the desired region."
      ],
      "metadata": {
        "id": "2_JjjLP0dw2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Vectorize reviews\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(negative_reviews_range_sorted['cleaned_content'])\n",
        "\n",
        "# Compute similarity between all reviews\n",
        "similarity_matrix = cosine_similarity(X)\n",
        "\n",
        "# Example: find reviews similar to review #0\n",
        "similar_indices = similarity_matrix[0].argsort()[::-1][1:6]  # top 5 similar reviews\n",
        "similar_reviews = negative_reviews_range_sorted.iloc[similar_indices]\n",
        "\n",
        "print(similar_reviews)"
      ],
      "metadata": {
        "id": "eE5PdjURbX3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ” Clustering & Thematic Analysis of Negative Reviews\n",
        "\n",
        "### 1. Text Extraction\n",
        "- Extract cleaned negative review texts from the DataFrame.\n",
        "- Convert them into a list for embedding generation.\n",
        "\n",
        "### 2. Sentence Embedding (Semantic Representation)\n",
        "- Use Sentence-BERT (`all-MiniLM-L6-v2`) to convert each review into dense vector embeddings.\n",
        "- Embeddings capture contextual semantic meaning beyond simple keyword matching.\n",
        "\n",
        "### 3. K-Means Clustering\n",
        "- Apply K-Means clustering with k = 5 to group semantically similar reviews.\n",
        "- Assign each review a cluster label and append it to the dataset.\n",
        "\n",
        "### 4. Top 3 Dominant Clusters Selection\n",
        "- Count reviews per cluster using `Counter`.\n",
        "- Select the top 3 clusters with the highest number of reviews for focused analysis.\n",
        "\n",
        "### 5. Cluster-Level Theme Extraction\n",
        "For each of the top 3 clusters:\n",
        "- Compute TF-IDF to extract top 10 representative keywords.\n",
        "- Calculate the cluster centroid in embedding space.\n",
        "- Measure cosine similarity between each review and the centroid.\n",
        "- Select the top 3 most representative reviews (closest to centroid).\n",
        "- Truncate long reviews for concise reporting.\n",
        "\n",
        "### 6. Cluster Summary Generation\n",
        "- Combine keywords and representative reviews into a structured summary.\n",
        "- Display cluster size, key themes, and sample reviews."
      ],
      "metadata": {
        "id": "QaZ1upyleo9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1ï¸âƒ£ Extract cleaned review texts\n",
        "texts = negative_reviews_range_sorted['cleaned_content'].tolist()\n",
        "\n",
        "# 2ï¸âƒ£ Create embeddings using Sentence-BERT\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "# 3ï¸âƒ£ Cluster embeddings\n",
        "num_clusters = 5  # try 5 clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Add cluster labels to DataFrame\n",
        "negative_reviews_range_sorted = negative_reviews_range_sorted.reset_index(drop=True)\n",
        "negative_reviews_range_sorted['cluster'] = labels\n",
        "\n",
        "# 4ï¸âƒ£ Find top 3 clusters by number of reviews\n",
        "cluster_counts = Counter(labels)\n",
        "top3_clusters = [cluster for cluster, _ in cluster_counts.most_common(3)]\n",
        "print(\"Top 3 clusters:\", top3_clusters)\n",
        "\n",
        "# 5ï¸âƒ£ Extract representative reviews & summary per cluster\n",
        "cluster_summaries = []\n",
        "\n",
        "for cluster in top3_clusters:\n",
        "    cluster_df = negative_reviews_range_sorted[negative_reviews_range_sorted['cluster']==cluster]\n",
        "    cluster_texts = cluster_df['cleaned_content'].tolist()\n",
        "\n",
        "    # TF-IDF top keywords for summary\n",
        "    tfidf = TfidfVectorizer(stop_words='english', max_features=10)\n",
        "    tfidf.fit(cluster_texts)\n",
        "    keywords = tfidf.get_feature_names_out()\n",
        "\n",
        "    # Compute cluster centroid and similarity to centroid\n",
        "    cluster_embeddings = embeddings[cluster_df.index]\n",
        "    centroid = cluster_embeddings.mean(axis=0)\n",
        "    sim_scores = cosine_similarity(cluster_embeddings, centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    # Pick top 3 most representative reviews\n",
        "    top_indices = sim_scores.argsort()[::-1][:3]\n",
        "    representative_reviews = [cluster_texts[i] for i in top_indices]\n",
        "\n",
        "    # Optional: truncate long reviews to first 2 sentences\n",
        "    truncated_reviews = ['. '.join(r.split('.')[:2]) + '.' for r in representative_reviews]\n",
        "\n",
        "    # Combine keywords + sample reviews for a brief summary\n",
        "    summary = f\"Cluster {cluster}: Keywords: {', '.join(keywords)}. Sample representative reviews: {truncated_reviews}\"\n",
        "\n",
        "    cluster_summaries.append({\n",
        "        'cluster': cluster,\n",
        "        'num_reviews': len(cluster_df),\n",
        "        'keywords': keywords,\n",
        "        'representative_reviews': truncated_reviews,\n",
        "        'summary': summary\n",
        "    })\n",
        "\n",
        "# 6ï¸âƒ£ Display summaries\n",
        "for cs in cluster_summaries:\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\"Cluster {cs['cluster']} | Number of reviews: {cs['num_reviews']}\")\n",
        "    print(\"Keywords:\", ', '.join(cs['keywords']))\n",
        "    print(\"Representative reviews:\")\n",
        "    for r in cs['representative_reviews']:\n",
        "        print(\"-\", r)\n",
        "    print(\"Summary:\")\n",
        "    print(cs['summary'])"
      ],
      "metadata": {
        "id": "9hBEIl29cfSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate word clouds only for top 3 clusters\n",
        "for cluster in top3_clusters:\n",
        "\n",
        "    cluster_df = negative_reviews_range_sorted[\n",
        "        negative_reviews_range_sorted['cluster'] == cluster\n",
        "    ]\n",
        "\n",
        "    cluster_texts = cluster_df['cleaned_content'].astype(str).tolist()\n",
        "    combined_text = \" \".join(cluster_texts)\n",
        "\n",
        "    # Optional: add custom stopwords\n",
        "    custom_stopwords = set(STOPWORDS)\n",
        "    custom_stopwords.update([\"app\", \"use\", \"get\"])  # add domain-generic words if needed\n",
        "\n",
        "    # Create word cloud\n",
        "    wordcloud = WordCloud(\n",
        "        width=900,\n",
        "        height=450,\n",
        "        background_color='white',\n",
        "        stopwords=custom_stopwords,\n",
        "        max_words=100\n",
        "    ).generate(combined_text)\n",
        "\n",
        "    # Plot one figure per cluster\n",
        "    plt.figure()\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud - Negative Review Cluster {cluster}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RQqNC4STgioX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Insights from Negative Reviews\n",
        "\n",
        "### 1. Flaws in the Energy System (Version 6+)\n",
        "A large portion of negative reviews highlight issues with the energy system. Free users frequently report that their progress is blocked due to depleted energy, leading to frustration and a feeling of stalled learning.\n",
        "\n",
        "### 2. AI Recommendations Are Problematic\n",
        "Users criticize the AI-driven lesson recommendations, reporting low accuracy and repetitive lesson suggestions. Many feel the AI does not adapt properly to their learning needs, resulting in a frustrating experience.\n",
        "\n",
        "### 3. Excessive Advertising Interruptions\n",
        "Many users find the frequency of ads disruptive. Unlike entertainment apps, Duolingo users are here to learn, so the ads interrupt their focus. This issue is often compounded by the energy system, as users feel forced to watch ads to continue progress."
      ],
      "metadata": {
        "id": "sjkz2x1AhYMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suggested App Improvements\n",
        "\n",
        "### 1. Revise the Energy System\n",
        "- Adjust the energy system so that energy is only depleted when mistakes are made, not simply by completing lessons.  \n",
        "- Increase energy limits for free users or provide alternative ways to earn energy through daily activities.  \n",
        "- Introduce a smoother progression system that reduces frustration from blocked lessons.  \n",
        "- Offer clear communication about energy consumption and refill options.\n",
        "\n",
        "### 2. Improve AI Lesson Recommendations\n",
        "- Enhance the AI algorithm to reduce repetition and better adapt to individual learning pace.  \n",
        "- Allow users to provide feedback on recommended lessons to refine AI suggestions.  \n",
        "- Introduce optional â€œlesson varietyâ€ settings so users can control lesson rotation.  \n",
        "- Add a setting for users to turn off AI recommendations entirely if they prefer.\n",
        "\n",
        "### 3. Reduce Advertising Disruption\n",
        "- Limit ad frequency for free users, especially when energy has been depleted.  \n",
        "- Provide a clear distinction between learning and optional ad engagement.  \n",
        "- Explore non-intrusive ad formats or incentives (e.g., optional ads for extra energy rather than forced interruptions)."
      ],
      "metadata": {
        "id": "6vic4VCqhUpj"
      }
    }
  ]
}